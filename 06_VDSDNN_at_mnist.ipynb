{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "follow from https://arxiv.org/abs/1701.05369, https://github.com/BayesWatch/tf-variational-dropout\n",
    "\n",
    "（I am not confident to follow successfully... To review again...）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ[\"CUDA_DEVICE_ORDER\"]    = \"PCI_BUS_ID\"\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "def get_available_devices():\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    return [x.name for x in local_device_protos]\n",
    "\n",
    "print(get_available_devices())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "from random import randint\n",
    "import time\n",
    "\n",
    "from ipywidgets import FloatProgress\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "import keras\n",
    "from   keras.datasets   import mnist\n",
    "\n",
    "from   tensorflow.examples.tutorials.mnist import input_data\n",
    "import tensorflow as tf\n",
    "\n",
    "eps = 1e-10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data load\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# reshape to 1 dim\n",
    "X_train      = X_train.reshape(np.shape(X_train)[0], \n",
    "                               (np.shape(X_train)[1] * np.shape(X_train)[2]))\n",
    "X_test       = X_test.reshape(np.shape(X_test)[0], \n",
    "                              (np.shape(X_test)[1] * np.shape(X_test)[2]))\n",
    "\n",
    "# change type\n",
    "X_train      = X_train.astype('float32')\n",
    "X_test       = X_test.astype('float32')\n",
    "\n",
    "# normalization\n",
    "X_train_mean = np.mean(X_train)\n",
    "X_train_std  = np.std(X_train)\n",
    "X_train      = (X_train - X_train_mean) / (X_train_std + eps)\n",
    "X_test       = (X_test  - X_train_mean) / (X_train_std + eps)\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "class_num    = np.max(y_train) + 1 # consider class 0 \n",
    "y_train      = keras.utils.to_categorical(y_train, class_num).astype('float32')\n",
    "y_test       = keras.utils.to_categorical(y_test,  class_num).astype('float32')\n",
    "\n",
    "print('np.shape(X_train) = (%d, %d)' % np.shape(X_train))\n",
    "print('np.shape(y_train) = (%d, %d)' % np.shape(y_train))\n",
    "print('np.shape(X_test)  = (%d, %d)' % np.shape(X_test))\n",
    "print('np.shape(y_test)  = (%d, %d)' % np.shape(y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set parameter for training\n",
    "learning_rate         = 1e-3\n",
    "epoch_num             = 50\n",
    "mini_batch_size       = int(128 * 0.5)\n",
    "mini_batch_iter       = 100 # if None then all train data use　on a epoch\n",
    "random_seed           = 0\n",
    "\n",
    "alpha___              = 0.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix mini batch iteration num\n",
    "train_data_num      = len(y_train)\n",
    "mini_batch_iter_tmp = round((train_data_num / mini_batch_size) - 0.5)\n",
    "if (mini_batch_iter is None):\n",
    "    mini_batch_iter = mini_batch_iter_tmp\n",
    "else:\n",
    "    if (mini_batch_iter > mini_batch_iter_tmp):\n",
    "        mini_batch_iter = mini_batch_iter_tmp\n",
    "        print('mini_batch_iter is too huge...')\n",
    "        print('suppress to mini_batch_iter = %d' % mini_batch_iter)\n",
    "        print('')\n",
    "\n",
    "print('mini_batch_iter = %d' % mini_batch_iter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ard_init      = -10.0\n",
    "thresh        = 3\n",
    "activation_fn = tf.nn.relu\n",
    "\n",
    "np.random.seed(random_seed)\n",
    "\n",
    "sess          = tf.InteractiveSession()\n",
    "\n",
    "# make model by tensorflow\n",
    "\n",
    "# input layer\n",
    "x             = tf.placeholder(dtype=tf.float32, shape=[None, 784])\n",
    "phase         = tf.placeholder(dtype=tf.bool, shape=None)\n",
    "\n",
    "\n",
    "# ---------- block 1 ----------\n",
    "\n",
    "# define weight\n",
    "weight_1      = tf.Variable(initial_value=tf.random_normal(shape=[784, 100], \n",
    "                                                          mean=0.0, stddev=0.01))\n",
    "bias_1        = tf.Variable(initial_value=tf.zeros(shape=[100]))\n",
    "\n",
    "# \n",
    "log_sigma2_1  = tf.Variable(initial_value=tf.constant(shape=[784, 100], \n",
    "                                                      value=ard_init))\n",
    "log_alpha_1   = tf.clip_by_value(t=(log_sigma2_1 - tf.log(tf.square(weight_1) + eps)),\n",
    "                                 clip_value_min=-8.0, clip_value_max=8.0)\n",
    "\n",
    "# at train time, adding noise\n",
    "mu_1          = tf.matmul(a=x, b=weight_1)\n",
    "si_1          = tf.sqrt(x=tf.matmul(a=tf.square(x), \n",
    "                                    b=(tf.exp(log_alpha_1) * tf.square(weight_1))) + eps)\n",
    "fc_noisy_1    = mu_1 + (si_1 * tf.random_normal(shape=tf.shape(mu_1), \n",
    "                                                mean=0.0, stddev=1.0))\n",
    "\n",
    "# at test time, we just mask\n",
    "select_mask_1 = tf.cast(x=tf.less(x=log_alpha_1, y=thresh), dtype=tf.float32)\n",
    "fc_masked_1   = tf.matmul(a=x, b=(weight_1 * select_mask_1))\n",
    "\n",
    "# choose between adding noise, or applying mask, depending on phase\n",
    "matmul_1      = tf.cond(pred=phase, true_fn=(lambda:fc_noisy_1), false_fn=(lambda:fc_masked_1))\n",
    "activations_1 = activation_fn(matmul_1 + bias_1)\n",
    "\n",
    "\n",
    "# ---------- block 2 ----------\n",
    "\n",
    "# define weight\n",
    "weight_2      = tf.Variable(initial_value=tf.random_normal(shape=[100, 10], \n",
    "                                                          mean=0.0, stddev=0.01))\n",
    "bias_2        = tf.Variable(initial_value=tf.zeros(shape=[10]))\n",
    "\n",
    "# \n",
    "log_sigma2_2  = tf.Variable(initial_value=tf.constant(shape=[100, 10], \n",
    "                                                      value=ard_init))\n",
    "log_alpha_2   = tf.clip_by_value(t=(log_sigma2_2 - tf.log(tf.square(weight_2) + eps)),\n",
    "                                 clip_value_min=-8.0, clip_value_max=8.0)\n",
    "\n",
    "# at train time, adding noise\n",
    "mu_2          = tf.matmul(a=activations_1, b=weight_2)\n",
    "si_2          = tf.sqrt(x=tf.matmul(a=tf.square(activations_1), \n",
    "                                    b=(tf.exp(log_alpha_2) * tf.square(weight_2))) + eps)\n",
    "fc_noisy_2    = mu_2 + (si_2 * tf.random_normal(shape=tf.shape(mu_2), \n",
    "                                                mean=0.0, stddev=1.0))\n",
    "\n",
    "# at test time, we just mask\n",
    "select_mask_2 = tf.cast(x=tf.less(x=log_alpha_2, y=thresh), dtype=tf.float32)\n",
    "fc_masked_2   = tf.matmul(a=activations_1, b=(weight_2 * select_mask_2))\n",
    "\n",
    "# choose between adding noise, or applying mask, depending on phase\n",
    "matmul_2      = tf.cond(pred=phase, true_fn=(lambda:fc_noisy_2), false_fn=(lambda:fc_masked_2))\n",
    "activations_2 = activation_fn(matmul_2 + bias_2)\n",
    "\n",
    "\n",
    "# output layer\n",
    "y             = tf.nn.softmax(logits=activations_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define loss function and optimizer\n",
    "y_            = tf.placeholder(dtype=tf.float32, shape=[None, 10])\n",
    "prediction    = tf.clip_by_value(t=y, clip_value_min=eps, clip_value_max=1.0)\n",
    "cross_entropy = - tf.reduce_mean(input_tensor=(y_ * tf.log(prediction)))\n",
    "\n",
    "# prior DKL part of the ELBO\n",
    "# utility to gather variational dropout parameters\n",
    "k1, k2, k3    = 0.63576, 1.8732, 1.48695\n",
    "C             = -k1\n",
    "\n",
    "mdkl_1        = (k1 * tf.nn.sigmoid(k2 + k3 * log_alpha_1)) - (0.5 * tf.log1p(tf.exp(-log_alpha_1))) + C\n",
    "dkl_qp_1      = -tf.reduce_sum(mdkl_1)\n",
    "mdkl_2        = (k1 * tf.nn.sigmoid(k2 + k3 * log_alpha_2)) - (0.5 * tf.log1p(tf.exp(-log_alpha_2))) + C\n",
    "dkl_qp_2      = -tf.reduce_sum(mdkl_2)\n",
    "\n",
    "divergences   = []\n",
    "divergences.append(dkl_qp_1)\n",
    "divergences.append(dkl_qp_2)\n",
    "\n",
    "# combine to form the ELBO\n",
    "N             = float(np.shape(X_train)[0])\n",
    "dkl           = tf.reduce_sum(tf.stack(divergences))\n",
    "elbo          = cross_entropy + alpha___ * (1.0 / N) * dkl\n",
    "\n",
    "# \n",
    "optimizer     = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "train_step    = optimizer.minimize(loss=elbo)\n",
    "\n",
    "tf.global_variables_initializer().run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# \n",
    "train_acc_stock  = np.zeros(epoch_num)\n",
    "test_acc_stock   = np.zeros(epoch_num)\n",
    "test_acc_best    = 0.0\n",
    "train_loss_stock = np.zeros(epoch_num)\n",
    "test_loss_stock  = np.zeros(epoch_num)\n",
    "\n",
    "fig              = plt.figure(figsize=(12,25),dpi=100)\n",
    "draw_pitch       = 5\n",
    "\n",
    "# training\n",
    "for epoch_i in range(epoch_num):\n",
    "    \n",
    "    # make index for mini batch selection\n",
    "    rand_idx    = np.random.permutation(len(y_train))\n",
    "    \n",
    "    # mini batch iteration (attentive　SGD)\n",
    "    for iter_i in range(mini_batch_iter):\n",
    "        \n",
    "        # set mini batch data\n",
    "        X_train_mini_batch = X_train[rand_idx[:mini_batch_size], :]\n",
    "        y_train_mini_batch = y_train[rand_idx[:mini_batch_size], :]\n",
    "        rand_idx           = rand_idx[mini_batch_size:]\n",
    "        \n",
    "        # learning\n",
    "        train_step.run(session=sess, \n",
    "                       feed_dict={x: X_train_mini_batch, \n",
    "                                  y_: y_train_mini_batch, \n",
    "                                  phase:True})\n",
    "        \n",
    "    # validaiton\n",
    "    correct_prediction        = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "    calc_acc                  = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    train_acc_stock[epoch_i]  = calc_acc.eval(feed_dict={x: X_train, y_: y_train, phase:False})\n",
    "    test_acc_stock[epoch_i]   = calc_acc.eval(feed_dict={x: X_test,  y_: y_test,  phase:False})\n",
    "    train_loss_stock[epoch_i] = sess.run(fetches=elbo, feed_dict={x: X_train, y_: y_train, phase:False})\n",
    "    test_loss_stock[epoch_i]  = sess.run(fetches=elbo, feed_dict={x: X_test,  y_: y_test,  phase:False})\n",
    "    \n",
    "    # best record update\n",
    "    if (test_acc_best < test_acc_stock[epoch_i]):\n",
    "        test_acc_best = test_acc_stock[epoch_i]\n",
    "    \n",
    "    if (((epoch_i + 1) % draw_pitch) == 0):\n",
    "        \n",
    "        # get now state\n",
    "        weight_1_tmp      = sess.run(weight_1)\n",
    "        weight_1_tmp_sort = np.sort(weight_1_tmp.flat[:])\n",
    "        log_alpha_1_tmp   = sess.run(log_alpha_1)\n",
    "        fc_noisy_1_tmp    = sess.run(fc_noisy_1,  \n",
    "                                     feed_dict={x:X_train_mini_batch, \n",
    "                                                weight_1:weight_1_tmp})\n",
    "        fc_masked_1_tmp   = sess.run(fc_masked_1, \n",
    "                                     feed_dict={x:X_train_mini_batch, \n",
    "                                                weight_1:weight_1_tmp})\n",
    "        \n",
    "        fc_noisy_1_tmp_   = fc_noisy_1_tmp.copy()\n",
    "        fc_masked_1_tmp_  = fc_masked_1_tmp.copy()\n",
    "        fc_noisy_1_tmp_[fc_noisy_1_tmp_ < 0]   = 0 # relu\n",
    "        fc_masked_1_tmp_[fc_masked_1_tmp_ < 0] = 0 # relu\n",
    "        \n",
    "        weight_2_tmp      = sess.run(weight_2)\n",
    "        weight_2_tmp_sort = np.sort(weight_2_tmp.flat[:])\n",
    "        log_alpha_2_tmp   = sess.run(log_alpha_2)\n",
    "        fc_noisy_2_tmp    = sess.run(fc_noisy_2,  \n",
    "                                     feed_dict={activations_1:fc_noisy_1_tmp_, \n",
    "                                                weight_2:weight_2_tmp})\n",
    "        fc_masked_2_tmp   = sess.run(fc_masked_2, \n",
    "                                     feed_dict={activations_1:fc_masked_1_tmp_, \n",
    "                                                weight_2:weight_2_tmp})\n",
    "        \n",
    "        # draw\n",
    "        ax_531 = fig.add_subplot(5, 3, 1)\n",
    "        ax_531.cla()\n",
    "        clear_output(wait=True)\n",
    "        ax_531.grid(which='major',color=[0.7, 0.7, 0.7],linestyle='-')\n",
    "        ax_531.imshow(weight_1_tmp, aspect='auto')\n",
    "        # plt.colorbar()\n",
    "        plt.title('imshow weight of 1st layer')\n",
    "        display(fig)\n",
    "\n",
    "        ax_532 = fig.add_subplot(5, 3, 2)\n",
    "        ax_532.cla()\n",
    "        clear_output(wait=True)\n",
    "        ax_532.grid(which='major',color=[0.7, 0.7, 0.7],linestyle='-')\n",
    "        ax_532.plot(weight_1_tmp_sort)\n",
    "        plt.title('plot weight of 1st layer\\n(sorted by value)')\n",
    "        display(fig)\n",
    "\n",
    "        ax_533 = fig.add_subplot(5, 3, 3)\n",
    "        ax_533.cla()\n",
    "        clear_output(wait=True)\n",
    "        ax_533.grid(which='major',color=[0.7, 0.7, 0.7],linestyle='-')\n",
    "        ax_533.hist(weight_1_tmp_sort, bins=50)\n",
    "        plt.yscale('log')\n",
    "        plt.title('histogram weight of 1st layer\\n(std : %.3f)' % np.std(weight_1_tmp))\n",
    "        display(fig)\n",
    "\n",
    "        ax_534 = fig.add_subplot(5, 3, 4)\n",
    "        ax_534.cla()\n",
    "        clear_output(wait=True)\n",
    "        ax_534.grid(which='major',color=[0.7, 0.7, 0.7],linestyle='-')\n",
    "        ax_534.imshow(log_alpha_1_tmp, aspect='auto')\n",
    "        # plt.colorbar()\n",
    "        plt.title('imshow weight of 1st layer')\n",
    "        display(fig)\n",
    "\n",
    "        ax_535 = fig.add_subplot(5, 3, 5)\n",
    "        ax_535.cla()\n",
    "        clear_output(wait=True)\n",
    "        ax_535.grid(which='major',color=[0.7, 0.7, 0.7],linestyle='-')\n",
    "        ax_535.imshow(fc_noisy_1_tmp, aspect='auto')\n",
    "        # plt.colorbar()\n",
    "        plt.title('imshow noisy of 1st layer')\n",
    "        display(fig)\n",
    "\n",
    "        ax_536 = fig.add_subplot(5, 3, 6)\n",
    "        ax_536.cla()\n",
    "        clear_output(wait=True)\n",
    "        ax_536.grid(which='major',color=[0.7, 0.7, 0.7],linestyle='-')\n",
    "        ax_536.imshow(fc_masked_1_tmp, aspect='auto')\n",
    "        # plt.colorbar()\n",
    "        plt.title('imshow masked of 1st layer')\n",
    "        display(fig)\n",
    "\n",
    "\n",
    "        ax_537 = fig.add_subplot(5, 3, 7)\n",
    "        ax_537.cla()\n",
    "        clear_output(wait=True)\n",
    "        ax_537.grid(which='major',color=[0.7, 0.7, 0.7],linestyle='-')\n",
    "        ax_537.imshow(weight_2_tmp, aspect='auto')\n",
    "        # plt.colorbar()\n",
    "        plt.title('imshow weight of 2nd layer')\n",
    "        display(fig)\n",
    "\n",
    "        ax_538 = fig.add_subplot(5, 3, 8)\n",
    "        ax_538.cla()\n",
    "        clear_output(wait=True)\n",
    "        ax_538.grid(which='major',color=[0.7, 0.7, 0.7],linestyle='-')\n",
    "        ax_538.plot(weight_2_tmp_sort)\n",
    "        plt.title('plot weight of 2nd layer\\n(sorted by value)')\n",
    "        display(fig)\n",
    "\n",
    "        ax_539 = fig.add_subplot(5, 3, 9)\n",
    "        ax_539.cla()\n",
    "        clear_output(wait=True)\n",
    "        ax_539.grid(which='major',color=[0.7, 0.7, 0.7],linestyle='-')\n",
    "        ax_539.hist(weight_2_tmp_sort, bins=50)\n",
    "        plt.yscale('log')\n",
    "        plt.title('histogram weight of 2nd layer\\n(std : %.3f)' % np.std(weight_1_tmp))\n",
    "        display(fig)\n",
    "\n",
    "        ax_53A = fig.add_subplot(5, 3, 10)\n",
    "        ax_53A.cla()\n",
    "        clear_output(wait=True)\n",
    "        ax_53A.grid(which='major',color=[0.7, 0.7, 0.7],linestyle='-')\n",
    "        ax_53A.imshow(log_alpha_2_tmp, aspect='auto')\n",
    "        # plt.colorbar()\n",
    "        plt.title('imshow weight of 2nd layer')\n",
    "        display(fig)\n",
    "\n",
    "        ax_53B = fig.add_subplot(5, 3, 11)\n",
    "        ax_53B.cla()\n",
    "        clear_output(wait=True)\n",
    "        ax_53B.grid(which='major',color=[0.7, 0.7, 0.7],linestyle='-')\n",
    "        ax_53B.imshow(fc_noisy_2_tmp, aspect='auto')\n",
    "        # plt.colorbar()\n",
    "        plt.title('imshow noisy of 2nd layer')\n",
    "        display(fig)\n",
    "\n",
    "        ax_53C = fig.add_subplot(5, 3, 12)\n",
    "        ax_53C.cla()\n",
    "        clear_output(wait=True)\n",
    "        ax_53C.grid(which='major',color=[0.7, 0.7, 0.7],linestyle='-')\n",
    "        ax_53C.imshow(fc_masked_2_tmp, aspect='auto')\n",
    "        # plt.colorbar()\n",
    "        plt.title('imshow masked of 2nd layer')\n",
    "        display(fig)\n",
    "\n",
    "\n",
    "        ax_529 = fig.add_subplot(5, 2, 9)\n",
    "        ax_529.cla()\n",
    "        clear_output(wait=True)\n",
    "        ax_529.grid(which='major',color=[0.7, 0.7, 0.7],linestyle='-')\n",
    "        ax_529.plot(train_acc_stock[:(epoch_i + 1)])\n",
    "        ax_529.plot(test_acc_stock[:(epoch_i + 1)])\n",
    "        plt.ylim(-0.05, 1.05)\n",
    "        plt.title('accuracy of train data = %.2f%%\\naccuracy of test data = %.2f%% (best : %.2f%%)' \\\n",
    "                  % ((train_acc_stock[epoch_i] * 100), (test_acc_stock[epoch_i] * 100), (test_acc_best * 100)))\n",
    "        display(fig)\n",
    "\n",
    "        ax_52A = fig.add_subplot(5, 2, 10)\n",
    "        ax_52A.cla()\n",
    "        clear_output(wait=True)\n",
    "        ax_52A.grid(which='major',color=[0.7, 0.7, 0.7],linestyle='-')\n",
    "        ax_52A.plot(train_loss_stock[:(epoch_i + 1)])\n",
    "        ax_52A.plot(test_loss_stock[:(epoch_i + 1)])\n",
    "        plt.ylim(-0.05, 1.05)\n",
    "        plt.title('loss of train data = %.5f\\nloss of test data = %.5f' \\\n",
    "                  % (train_loss_stock[epoch_i], test_loss_stock[epoch_i]))\n",
    "        display(fig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
